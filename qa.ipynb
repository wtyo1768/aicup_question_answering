{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('aicup': conda)"
  },
  "interpreter": {
   "hash": "ca5915a2c02dea2c8acf4e447856dc800933eee93a34a922b88f60d8ef1a4460"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: features.msgpack does not exist, try loading features.pkl\n"
     ]
    }
   ],
   "source": [
    "from loader import *\n",
    "from fastNLP.io import ChnSentiCorpLoader\n",
    "from fastNLP import DataSet\n",
    "from fastNLP import Vocabulary\n",
    "from fastNLP.io import ChnSentiCorpPipe, DataBundle\n",
    "from spacy.lang.zh import Chinese\n",
    "import spacy\n",
    "from fastNLP.embeddings import StaticEmbedding, StackEmbedding\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import re\n",
    "\n",
    "# nlp = Chinese()\n",
    "# nlp = Chinese.from_config({\"nlp\": {\"tokenizer\":{\"segmenter\": \"pkuseg\"}}})\n",
    "# nlp.tokenizer.initialize(pkuseg_model=\"mixed\", pkuseg_user_dict='./data/dict.txt')\n",
    "from prepare_data import nlp, role_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, q, cho, ans = json_parser('./data/val_fold1.json')\n",
    "raw_chars = {\n",
    "        'raw_text': doc,\n",
    "        'cho'     : cho,\n",
    "        'q'       : q,\n",
    "}\n",
    "ds = DataSet(raw_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<2>反正至少碰面就是會是需要，<2>就是講好的嗎？<1>對。<2>那所以你怎麼決定我這次碰面要吃還是不要吃？<1>就……<1>他如果想要的話，<1>我就會先吃啊。<2>那假設是說，<2>誒不預期這次會不會有可能，<2>就是原本有沒有跟他講好。<1>嗯。<2>通常我們有時候會建議說，<2>如果真的不預期性很大的話，<2>那兩顆你就先吃。<1>嗯。<1>嗯。<2>那如果最後真的沒發生事情……<1>就算了。<2>那你後面兩顆就不用吃嘛。<1>那如果比如說是五六日好了，<1>那我禮拜五吃了嘛，<1>兩顆一顆一顆，<2>對不對？<1>嗯。<2>前面二到兩顆，<2>前面至少兩個小時吃兩顆嘛對不對？<1>嗯。<2>然後你後面這邊下一天再吃一顆，<2>下一天再吃一顆嘛，'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "doc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['',\n",
       " '2>反正至少碰面就是會是需要，',\n",
       " '2>就是講好的嗎？',\n",
       " '1>對。',\n",
       " '2>那所以你怎麼決定我這次碰面要吃還是不要吃？',\n",
       " '1>就……',\n",
       " '1>他如果想要的話，',\n",
       " '1>我就會先吃啊。',\n",
       " '2>那假設是說，',\n",
       " '2>誒不預期這次會不會有可能，',\n",
       " '2>就是原本有沒有跟他講好。',\n",
       " '1>嗯。',\n",
       " '2>通常我們有時候會建議說，',\n",
       " '2>如果真的不預期性很大的話，',\n",
       " '2>那兩顆你就先吃。',\n",
       " '1>嗯。',\n",
       " '1>嗯。',\n",
       " '2>那如果最後真的沒發生事情……',\n",
       " '1>就算了。',\n",
       " '2>那你後面兩顆就不用吃嘛。',\n",
       " '1>那如果比如說是五六日好了，',\n",
       " '1>那我禮拜五吃了嘛，',\n",
       " '1>兩顆一顆一顆，',\n",
       " '2>對不對？',\n",
       " '1>嗯。',\n",
       " '2>前面二到兩顆，',\n",
       " '2>前面至少兩個小時吃兩顆嘛對不對？',\n",
       " '1>嗯。',\n",
       " '2>然後你後面這邊下一天再吃一顆，',\n",
       " '2>下一天再吃一顆嘛，']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "doc[0][0].split('<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+-------------------------------+-------------------------------+-------------------------------+\n",
       "| raw_text                      | cho                           | q                             |\n",
       "+-------------------------------+-------------------------------+-------------------------------+\n",
       "| ['<2>反正至少碰面就是會是...  | ['至少兩個小時吃兩顆', '禮... | 請問醫師建議民眾藥要怎麼吃... |\n",
       "| ['<2>嗯哼。<1>對，<1>可是...  | ['民眾', '伴侶', '兩個人都... | 下列何者人物是感染者？        |\n",
       "| ['<2>是這樣吧？<1>嗯。<2>...  | ['吃藥', '抽血', '車禍']      | 請問民眾為什麼上個月開始在... |\n",
       "| ['<2>概念上是這個樣子啦齁...  | ['方便', '會比較花錢', '要... | 下列何者不是天天吃藥這種吃... |\n",
       "| ['<2>以前約過的？<1>呵呵呵... | ['有戴保險套', 'APP上約的...  | 下列何者敘述關於民眾最近的... |\n",
       "| ['<2>最近還好嗎？<1>還好阿... | ['都會帶', '都無套', '一兩... | 請問民眾使用保險套的次數為... |\n",
       "| ['<4>對，<4>通常，<4>那你...  | ['吃PrEP', '無套', '戴套'...  | 請問個管師建議民眾保護自己... |\n",
       "| ['<4>你還好嗎？<1>還好。<...  | ['民眾都會戴套', '民眾這個... | 下列敘述何錯誤?               |\n",
       "| ['<1>最近……<1>最近沒有吃...   | ['匿篩，抽了陰性', '性行為... | 為什麼民眾這幾個月就都沒有... |\n",
       "| ['<2>有戴套，<2>還是有戴就... | ['第一劑', '第二劑', '第三... | 請問民眾HPV疫苗已施打完第...  |\n",
       "| ['<2>真的喔，<2>戒多久了？... | ['有一種很討厭的感覺', '不... | 請問關於民眾停止用藥的原因... |\n",
       "| ['<2>擔心就是這個結果會、...  | ['HIV一定不會得', '是感染...  | 下列何者關於民眾的敘述是正... |\n",
       "| ...                           | ...                           | ...                           |\n",
       "+-------------------------------+-------------------------------+-------------------------------+"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(ins):\n",
    "    role_ins = []\n",
    "    ins_frag = []\n",
    "    for frag in ins:\n",
    "        roleid = []\n",
    "        tokenize_sen = []\n",
    "        sents = doc[0][0].split('<')[1:]\n",
    "        for s in sents:\n",
    "            sid =s[0]\n",
    "            # print(s[3:], '\\n')\n",
    "            s = [ele.text for ele in nlp.tokenizer.pipe(s[2:])]\n",
    "            roleid += [sid] * len(s)\n",
    "            tokenize_sen+=s\n",
    "\n",
    "        role_ins.append(roleid)\n",
    "        ins_frag.append(tokenize_sen)\n",
    "        assert len(roleid) == len(tokenize_sen)\n",
    "    return {\n",
    "        'raw_text': ins_frag,\n",
    "        'roleid' :role_ins,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bined_query_cho = False\n",
    "\n",
    "\n",
    "def build_ds(f):\n",
    "    doc, q, cho, ans = json_parser(f)\n",
    "    if bined_query_cho :\n",
    "        cho = [[q[i]+ele for ele in sublist] for i, sublist in enumerate(cho) ]\n",
    "    raw_chars = {\n",
    "        'raw_text': doc,\n",
    "        'cho'     : cho,\n",
    "        'q'       : q,\n",
    "    }\n",
    "    if not ans == []:\n",
    "        ans= [ele['qa'] for ele in ans] \n",
    "        ans_onehot =[np.eye(3)[ele] for ele in ans]\n",
    "        raw_chars.update({'target':ans, 'target_onehot':ans_onehot})\n",
    "\n",
    "    ds = DataSet(raw_chars)\n",
    "    ds.apply_field_more(tokenize, field_name='raw_text', is_input=True,)\n",
    "    # for k in ['raw_text', 'cho']:\n",
    "    ds.apply_field(\n",
    "        lambda ins: [[ele.text for ele in nlp.tokenizer.pipe(sublist)] for sublist in ins], \n",
    "        field_name='cho',\n",
    "        new_field_name='cho', \n",
    "        is_input=True\n",
    "        )\n",
    "    ds.apply_field(\n",
    "        lambda ins: [ele.text for ele in nlp.tokenizer.pipe(ins)], \n",
    "        field_name='q',\n",
    "        new_field_name='q', \n",
    "        is_input=True\n",
    "    )    \n",
    "    if not ans==[]:\n",
    "        ds.set_input('target_onehot')\n",
    "        ds.set_target('target') \n",
    "    return ds   \n",
    "\n",
    "\n",
    "ds = {}\n",
    "train = './data/train_foldidx.json'\n",
    "val = './data/val_foldidx.json'\n",
    "\n",
    "for i in range(5):\n",
    "    ds.update({\n",
    "        f'train{i}' : build_ds(train.replace('idx', str(i))),\n",
    "        f'val{i}'   : build_ds(val.replace('idx', str(i))),\n",
    "    })\n",
    "ds['dev'] = build_ds('./data/Develop_bined.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+------------------+------------------+------------------+--------+---------------+------------------+\n",
       "| raw_text         | cho              | q                | target | target_onehot | roleid           |\n",
       "+------------------+------------------+------------------+--------+---------------+------------------+\n",
       "| [[82, 53, 45,... | [[176, 902, 2... | [31, 101, 99,... | 0      | [1. 0. 0.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[45, 40, 11,... | [104, 102, 11... | 0      | [1. 0. 0.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[765, 766], ... | [118, 119, 54... | 1      | [0. 1. 0.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[303, 186, 1... | [104, 102, 97... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[97, 98, 4, ... | [31, 101, 123... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[767, 499, 7... | [31, 101, 168... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[668, 943, 7... | [31, 101, 99,... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[97, 98], [1... | [31, 101, 99,... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[68, 359, 60... | [97, 98, 13, ... | 0      | [1. 0. 0.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[4, 108, 132... | [104, 102, 31... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[64, 51, 7, ... | [97, 98, 13, ... | 2      | [0. 0. 1.]    | [['2', '2', '... |\n",
       "| [[82, 53, 45,... | [[52, 174], [... | [105, 112, 97... | 0      | [1. 0. 0.]    | [['2', '2', '... |\n",
       "| ...              | ...              | ...              | ...    | ...           | ...              |\n",
       "+------------------+------------------+------------------+--------+---------------+------------------+"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "ds['train3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary([0, 1]...)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "raw_key = ['raw_text', 'cho', 'q']\n",
    "\n",
    "label_ds = DataSet({'target':[0, 1]})\n",
    "label_vocab  = Vocabulary(unknown=None, padding=None).from_dataset(label_ds, field_name='target')\n",
    "char_vocab = Vocabulary()\n",
    "char_vocab.from_dataset(*ds.values(), field_name=raw_key\n",
    "    ,no_create_entry_dataset=[ds['dev']]\n",
    ")\n",
    "char_vocab.index_dataset(*ds.values(), field_name=raw_key)\n",
    "\n",
    "label_vocab.from_dataset(label_ds, field_name='target')\n",
    "label_vocab.index_dataset(label_ds, field_name='target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "char_vocab.to_word(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = DataBundle({ 'target':label_vocab}, ds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "set(role_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "torch.tensor(np.array(['1']).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from fastNLP.modules import LSTM\n",
    "import torch\n",
    "from fastNLP.models.cnn_text_classification import CNNText\n",
    "\n",
    "\n",
    "class BiLSTMMaxPoolCls(nn.Module):\n",
    "    def __init__(self, embed, d_model=300, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = embed\n",
    "        self.num_class = 2 \n",
    "        self.d_model = d_model\n",
    "        self.dropout = .2\n",
    "        num_head = 4\n",
    "        roleid_size = len(set(role_map.values())) + 1 # pad\n",
    "\n",
    "        self.role_emb = nn.Embedding(roleid_size, d_model)\n",
    "        self.emb_enc = EncoderBlock(conv_num=4, d_model=d_model, num_head=num_head, k=7, dropout=0.1)\n",
    "        self.c_att = nn.MultiheadAttention(d_model, num_head, dropout=self.dropout)\n",
    "        self.q_att = nn.MultiheadAttention(d_model, num_head, dropout=self.dropout)\n",
    "\n",
    "        self.lstm = LSTM(d_model, hidden_size=d_model//2, num_layers=num_layers,\n",
    "                         batch_first=True, bidirectional=True)\n",
    "        self.bilstm = LSTM(d_model, hidden_size=d_model//2, num_layers=num_layers,\n",
    "                         batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(d_model, self.num_class)\n",
    "\n",
    "\n",
    "    def forward(self, raw_text, q, cho, roleid, target_onehot=None):  \n",
    "        PAD = 0\n",
    "        bsz = raw_text.size(0)\n",
    "        d_len, q_len, c_len = raw_text.size(-1), q.size(-1), cho.size(-1)\n",
    "\n",
    "        d_mask = (torch.ones_like(raw_text)*(raw_text==PAD)).bool().view(bsz, 3, d_len)\n",
    "        q_mask = (torch.ones_like(q)*(q==PAD)).bool().view(bsz, q_len)\n",
    "        c_mask = (torch.ones_like(cho)*(cho==PAD)).bool().view(bsz, 3, c_len)\n",
    "\n",
    "        raw_text = raw_text.view(3*bsz, d_len)\n",
    "        cho = cho.view(3*bsz, c_len)\n",
    "\n",
    "        \n",
    "        d_emb = self.embed(raw_text).view(bsz, 3, d_len, self.d_model)\n",
    "        q_emb = self.embed(q).view(bsz, q_len, self.d_model)\n",
    "        c_emb = self.embed(cho).view(bsz, 3, c_len, self.d_model)\n",
    "\n",
    "        role_emb = self.role_emb(torch.from_numpy(roleid.astype(int)).to(device)).view(bsz, 3, d_len, self.d_model)\n",
    "        # print(role_emb.shape, role_emb.dtype)\n",
    "        d_emb = d_emb + role_emb\n",
    "\n",
    "        c = [ele.squeeze(1).transpose(0, 1) for ele in torch.split(c_emb, 1, dim=1)]\n",
    "        d = [ele.squeeze(1).transpose(0, 1) for ele in torch.split(d_emb, 1, dim=1)]\n",
    "        d_mask = [ele.squeeze(1) for ele in torch.split(d_mask, 1, dim=1)]\n",
    "        c_mask = [ele.squeeze(1) for ele in torch.split(c_mask, 1, dim=1)]\n",
    "\n",
    "        Edc  = [self.c_att(c[i], d[i], d[i], key_padding_mask=d_mask[i])[0] \n",
    "                for i in range(3)]\n",
    "        Eqdc = [self.q_att(q_emb.transpose(0, 1), Edc[i], Edc[i], key_padding_mask=c_mask[i])[0].transpose(0, 1) \n",
    "                for i in range(3)]\n",
    "\n",
    "        Eqdc = torch.stack(Eqdc, dim=1)\n",
    "        # print(Eqdc.shape)\n",
    "        Eqdc = Eqdc.view(bsz*3, q_len, self.d_model)\n",
    "\n",
    "        outputs, _ = self.lstm(Eqdc)\n",
    "        outputs = outputs[:, -1, :]\n",
    "        outputs = outputs.view(bsz, 3, self.d_model)\n",
    "        \n",
    "        outputs, _ = self.bilstm(outputs)\n",
    "        outputs = self.fc(outputs)\n",
    "\n",
    "        if not target_onehot==None:\n",
    "            loss_fct = nn.CrossEntropyLoss(\n",
    "                reduction='sum', \n",
    "                weight=torch.tensor([0.66, 1.23]).to(outputs),\n",
    "            )\n",
    "            outputs = outputs.view(-1, self.num_class)\n",
    "            # print(outputs.shape, target_onehot.view(-1).shape)\n",
    "            # print(outputs.dtype)\n",
    "            loss = loss_fct(outputs, target_onehot.view(-1).to(torch.long)) / bsz\n",
    "            pred = torch.max(outputs.view(bsz, 3, self.num_class), dim=1).indices[:, 1]\n",
    "            return {'pred':pred, 'loss':loss} \n",
    "\n",
    "        pred = torch.max(outputs.view(bsz, 3, self.num_class), dim=1).indices[:, 1]\n",
    "        return {'pred':pred} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastNLP.embeddings import BertEmbedding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# embed = StackEmbedding([\n",
    "#     StaticEmbedding(char_vocab, model_dir_or_name='cn-fasttext'),\n",
    "#     # StaticEmbedding(char_vocab, model_dir_or_name='cn-sgns-literature-word')\n",
    "# ])\n",
    "# embed = BertEmbedding(\n",
    "#     char_vocab, model_dir_or_name='cn-wwm-ext', layers='1')\n",
    "\n",
    "# print('Embedding shape', embed.embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1267 out of 1274 words in the pre-training embedding.\nEmbedding shape 300\n--------Fold0----------\ninput fields after batch(if batch size is 2):\n\traw_text: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 3, 245]) \n\tcho: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 3, 7]) \n\tq: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 21]) \n\ttarget_onehot: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2, 3]) \n\troleid: (1)type:numpy.ndarray (2)dtype:<U1, (3)shape:(2, 3, 245) \ntarget fields after batch(if batch size is 2):\n\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n\ntraining epochs started 2021-06-14-16-58-56-547758\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1065 [00:00<?, ?it/s, loss:{0:<6.5f}]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cd2b67abcd84e9b87e6fb421cb873af"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "983d1c13ab584bbabc3c755e4646ca9d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 1/15. Step:71/1065: \nAccuracyMetric: acc=0.423077\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a3ecd754af14a4f8059a97934df15a6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 2/15. Step:142/1065: \nAccuracyMetric: acc=0.423077\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5900279e1c454905bac133e5430f4d0d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 3/15. Step:213/1065: \nAccuracyMetric: acc=0.423077\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27ddf44a71c948e3a122b2ebc7c97678"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.09 seconds!\nEvaluation on dev at Epoch 4/15. Step:284/1065: \nAccuracyMetric: acc=0.423077\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13134b28413e4fe9897abf9ad7313e08"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.09 seconds!\nEvaluation on dev at Epoch 5/15. Step:355/1065: \nAccuracyMetric: acc=0.423077\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec97de163141443f96e07739541d59f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 6/15. Step:426/1065: \nAccuracyMetric: acc=0.415385\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f9236118cbc422e9b0499d09cae2d22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 7/15. Step:497/1065: \nAccuracyMetric: acc=0.407692\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00bd6190b70d44eaa69a001107735320"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.09 seconds!\nEvaluation on dev at Epoch 8/15. Step:568/1065: \nAccuracyMetric: acc=0.407692\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3982f6c93c3a4d888cd5950a319734c3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 9/15. Step:639/1065: \nAccuracyMetric: acc=0.392308\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9342e54a26741a09395ea3b62dcbe73"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 10/15. Step:710/1065: \nAccuracyMetric: acc=0.415385\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14f532d6abc64099b4e03966309e6f1b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 11/15. Step:781/1065: \nAccuracyMetric: acc=0.430769\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c69c6b729e134c90859e0f4e544a0d71"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.09 seconds!\nEvaluation on dev at Epoch 12/15. Step:852/1065: \nAccuracyMetric: acc=0.423077\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75d684bf624c4f46b0f9445cdc8b79d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 13/15. Step:923/1065: \nAccuracyMetric: acc=0.430769\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4361122f760846caa8d59f12928ac813"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.1 seconds!\nEvaluation on dev at Epoch 14/15. Step:994/1065: \nAccuracyMetric: acc=0.430769\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb000853075240a48608dd845c5b2321"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate data in 0.09 seconds!\nEvaluation on dev at Epoch 15/15. Step:1065/1065: \nAccuracyMetric: acc=0.430769\n\nReloaded the best model.\n\nIn Epoch:11/Step:781, got best dev performance:\nAccuracyMetric: acc=0.430769\n"
     ]
    }
   ],
   "source": [
    "from fastNLP import Trainer\n",
    "from fastNLP import CrossEntropyLoss\n",
    "from torch.optim import Adam, AdamW\n",
    "from fastNLP import AccuracyMetric\n",
    "from qanet import *\n",
    "from fastNLP.core.callback import WarmupCallback,GradientClipCallback,EarlyStopCallback,SaveModelCallback\n",
    "from fastNLP import FitlogCallback\n",
    "from fastNLP import LRScheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from fastNLP.core import Callback\n",
    "from fastNLP.core.callback import WarmupCallback,GradientClipCallback,EarlyStopCallback,SaveModelCallback\n",
    "\n",
    "\n",
    "embed = StackEmbedding([\n",
    "    StaticEmbedding(char_vocab, model_dir_or_name='cn-fasttext'),\n",
    "    # StaticEmbedding(char_vocab, model_dir_or_name='cn-sgns-literature-word'),\n",
    "    # BertEmbedding(char_vocab, model_dir_or_name='cn-wwm-ext', layers='1'),\n",
    "\n",
    "])\n",
    "\n",
    "# embed = StackEmbedding([StaticEmbedding(char_vocab, model_dir_or_name='cn-fasttext')])\n",
    "print('Embedding shape', embed.embed_size)\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "metric = AccuracyMetric()\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for idx in range(1):\n",
    "  print(f'--------Fold{idx}----------')\n",
    "\n",
    "  \n",
    "  model = BiLSTMMaxPoolCls(embed, d_model=embed.embed_size)\n",
    "#   model = CNNText(embed, 2, dropout=0.2)\n",
    "  optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01,)\n",
    "  # lrschedule_callback = LRScheduler(lr_scheduler=LambdaLR(optimizer, lambda ep: 1 / (1 + 0.05*ep) ))\n",
    "  clip_callback = GradientClipCallback(clip_type='value', clip_value=2)\n",
    " \n",
    "  trainer = Trainer(\n",
    "    train_data=bundle.get_dataset(f'train{idx}'), \n",
    "    dev_data=bundle.get_dataset(f'val{idx}'),\n",
    "    model=model, device=device,\n",
    "    optimizer=optimizer, batch_size=8, n_epochs=15,\n",
    "    metrics=metric,\n",
    "    callbacks=[\n",
    "      # lrschedule_callback,\n",
    "      clip_callback,\n",
    "      WarmupCallback(warmup=50,),\n",
    "      # EarlyStopCallback(3),\n",
    "    ]       \n",
    "  )\n",
    "  trainer.train()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0, 1, 0, 0, 0, 2, 1, 0, 2, 1, 2, 2, 1, 0, 2, 2, 2, 1, 1, 0],\n",
       " [1, 2, 2, 1, 2, 0, 2, 2, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 2, 2],\n",
       " [2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 1, 1, 0, 0, 2, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "from fastNLP.core.predictor import Predictor\n",
    "pred = Predictor(model).predict(bundle.get_dataset('val0'))['pred']\n",
    "pred[0:20],pred[20:40],pred[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4153846153846154"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "a = np.array(list(bundle.get_dataset('val0')['target']))\n",
    "pred = np.array(pred)\n",
    "np.count_nonzero(a==pred) / a.shape[0]\n"
   ]
  }
 ]
}